{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propaganda Sequence Identification Project\n",
    "## Identifying and classifying propaganda sequences in news articles using a custom deep learning pipeline based on token classification.\n",
    "\n",
    "### Introduction\n",
    "The increasing popularity of online news and social media has heightened the need to automatically detect propaganda in digital content. While much prior research focused primarily on document-level analysis, more granular approaches are necessary to identify specific instances of propaganda techniques within texts.\n",
    "This project explores the application of transfer learning using DistilBERT to address two key challenges in propaganda detection: identifying text spans containing propaganda and classifying the specific propaganda technique employed. Building on the framework of SemEval-2020 Task 11, developed by Da San Martino et al. (2020), we leverage a dataset of news articles manually annotated by professional fact-checkers across 14 propaganda categories.\n",
    "Our implementation fine-tunes the distilbert-base-uncased model for both span identification and technique classification tasks. This fine-grained approach enables detailed analysis of how propaganda manifests within news content, rather than merely flagging entire articles as propagandistic or not. Such capabilities could support journalists, fact-checkers, and researchers in studying media manipulation, while also helping readers critically evaluate their news sources.\n",
    "The growing popularity of services like Ground News and projects like Tanbih highlights a rising public interest in understanding media bias. Tools capable of detecting and classifying specific propaganda techniques could provide valuable insights for consumers seeking to better understand the information landscape. This notebook demonstrates our methodology for implementing and evaluating transformer-based models to create such tools, which could add significant value for both media professionals and the general public.\n",
    "\n",
    "First, all imports necessary for the project. For some CUDA dependencies it is required to use Python version 3.11.5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Optional, Dict, Any, List, Tuple\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_recall_fscore_support, \n",
    "    classification_report\n",
    ")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, EvalPrediction\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Our Architecture:](report.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a class PropagandaDetector to access all the functions, models and paths and tokenizer necessary for Task 1.\n",
    "To initialize, it receives a model name and an output directory where the trained models are stored. Both can be customized, we achieved the best performance with the distilbert-base-uncased model so that is chosen as a default. Additionally, the max_span_length sets the length for the tokenizer, in this case meaning that articles that are longer than 512 huggingface wordpiece tokens will only have the first 512 tokens evaluated for training and prediction, the rest is truncated. For our purposes this is enough, there were no articles in our dataset that exceeded this length, however this is something to be aware of when trying to evaluate longer articles. The class is designed to This pipeline was designed to preprocess news articles into tokenized input suitable for model training, train a deep learning model to classify text spans as propaganda or non-propaganda and evaluate the model using standard metrics like precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods:\n",
    "Data Preprocessing\n",
    "\n",
    "Dataset Overview:\n",
    "The dataset comprises news articles labeled with propaganda spans. These spans are annotated with start and end character indices, denoting propaganda regions. The preprocessing steps convert these indices into token-level labels compatible with the transformer model.\n",
    "\n",
    "Key Preprocessing Steps:\n",
    "\n",
    "    Extract Word Labels:\n",
    "        Aligns propaganda spans with tokenized text.\n",
    "        Ensures padding tokens are ignored during training.\n",
    "    Load Dataset:\n",
    "        Reads labeled data from directories, tokenizes text, and aligns spans with tokens.\n",
    "        Utilizes Hugging Face’s Dataset object for efficient handling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Configuration\n",
    "\n",
    "The project employs distilBERT as the base model, fine-tuned for token classification.\n",
    "\n",
    "Model Architecture:\n",
    "\n",
    "    Token classification head with two labels: Propaganda and Non-Propaganda.\n",
    "    Pre-trained embeddings from distilBERT for efficient and effective learning.\n",
    "\n",
    "Training Pipeline:\n",
    "\n",
    "    Dynamic learning rate adjustment using ReduceLROnPlateau.\n",
    "    Gradient accumulation for memory efficiency on large batches.\n",
    "\n",
    "Metrics:\n",
    "\n",
    "    Accuracy: Measures overall correctness.\n",
    "    Precision & Recall: Evaluates the trade-off between false positives and false negatives.\n",
    "    F1 Score: Balances precision and recall for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Details\n",
    "\n",
    "    Split Dataset:\n",
    "        The dataset is split into training (90%) and validation (10%) sets.\n",
    "    Optimizer:\n",
    "        AdamW optimizer with an initial learning rate of 5e-5.\n",
    "    Batch Size:\n",
    "        Adaptive batch size depending on hardware capabilities (e.g., CUDA availability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropagandaDetector:\n",
    "    def __init__(self, \n",
    "                 model_name: str = 'distilbert-base-uncased', \n",
    "                 output_dir: str = \"models/output\",\n",
    "                 max_span_length: int = 512,\n",
    "                 resume_from_checkpoint: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize the Propaganda Detector.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Base model to use\n",
    "            output_dir (str): Directory to save model and outputs\n",
    "            max_span_length (int): Maximum length of text spans\n",
    "            resume_from_checkpoint (str, optional): Path to checkpoint to resume training\n",
    "        \"\"\"\n",
    "        # Check if CUDA is available and set device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"CUDA Device Capability: {torch.cuda.get_device_capability(0)}\")\n",
    "\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        self.max_span_length = max_span_length\n",
    "\n",
    "        # Initialize tokenizer\n",
    "        if resume_from_checkpoint:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(resume_from_checkpoint)\n",
    "        else:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "\n",
    "        # Initialize or load model\n",
    "        if resume_from_checkpoint:\n",
    "            self.model = AutoModelForTokenClassification.from_pretrained(\n",
    "                resume_from_checkpoint, num_labels=2\n",
    "            )\n",
    "        else:\n",
    "            self.model = AutoModelForTokenClassification.from_pretrained(\n",
    "                model_name, num_labels=2\n",
    "            )\n",
    "        \n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def extract_word_labels(self, text: str, propaganda_spans: List[Tuple[int, int]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Convert propaganda spans to word-level labels.\n",
    "\n",
    "        Args:\n",
    "            text (str): Full text content.\n",
    "            propaganda_spans (List[Tuple[int, int]]): List of start and end indices for propaganda spans.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary with tokenized `input_ids` and aligned `labels`.\n",
    "        \"\"\"\n",
    "        # Tokenize the text\n",
    "        tokenized = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_span_length,\n",
    "            return_offsets_mapping=True  # Get offsets for alignment\n",
    "        )\n",
    "\n",
    "        # Initialize labels\n",
    "        labels = [0] * len(tokenized[\"input_ids\"])  # Default to non-propaganda (0)\n",
    "\n",
    "        # Align spans with tokens\n",
    "        for start, end in propaganda_spans:\n",
    "            for idx, (token_start, token_end) in enumerate(tokenized[\"offset_mapping\"]):\n",
    "                if token_start >= start and token_end <= end:  # Token falls within a propaganda span\n",
    "                    labels[idx] = 1  # Propaganda\n",
    "        \n",
    "        # Ensure padding tokens are ignored during training\n",
    "        for idx, token_id in enumerate(tokenized[\"input_ids\"]):\n",
    "            if token_id == self.tokenizer.pad_token_id:\n",
    "                labels[idx] = -100  # Ignore padding tokens during loss computation\n",
    "\n",
    "        # Remove offset mapping (not needed for training)\n",
    "        tokenized.pop(\"offset_mapping\")\n",
    "\n",
    "        # Add labels to tokenized data\n",
    "        tokenized[\"labels\"] = labels\n",
    "        return tokenized\n",
    "\n",
    "\n",
    "    def load_data(self, articles_dir: str, train_labels_dir: str) -> Dataset:\n",
    "        \"\"\"\n",
    "        Loads articles and generates token-level labels from propaganda spans. \n",
    "\n",
    "        Args:\n",
    "         articles_dir (str): Directory containing article files.\n",
    "         train_labels_dir (str): Path to labels file.\n",
    "\n",
    "        Returns:\n",
    "            A HuggingFace Dataset object with tokenized input and aligned labels.\n",
    "        \"\"\"\n",
    "        data = []\n",
    "\n",
    "        # Read all label spans from the labels file\n",
    "        article_labels = {}\n",
    "        with open(train_labels_dir, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) == 3:\n",
    "                    article_id, start, end = parts[0], int(parts[1]), int(parts[2])\n",
    "                    if article_id not in article_labels:\n",
    "                        article_labels[article_id] = []\n",
    "                    article_labels[article_id].append((start, end))\n",
    "\n",
    "        # Process each article\n",
    "        for article_file in os.listdir(articles_dir):\n",
    "            # Ensure the file matches the expected naming pattern\n",
    "            if not article_file.startswith('article') or not article_file.endswith('.txt'):\n",
    "                continue\n",
    "\n",
    "            # Extract article ID\n",
    "            article_id = article_file[7:-4]  # Remove 'article' prefix and '.txt' suffix\n",
    "            article_path = os.path.join(articles_dir, article_file)\n",
    "\n",
    "            # Read article text\n",
    "            with open(article_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "\n",
    "            # If there are labels for this article\n",
    "            if article_id in article_labels:\n",
    "                propaganda_spans = article_labels[article_id]\n",
    "                tokenized_data = self.extract_word_labels(text, propaganda_spans)\n",
    "                data.append(tokenized_data)\n",
    "\n",
    "        if not data:\n",
    "            raise ValueError(\"No data loaded. Check dataset paths.\")\n",
    "\n",
    "        # Return a HuggingFace Dataset\n",
    "        return Dataset.from_list(data)\n",
    "\n",
    "\n",
    "    def tokenize_data(self, examples):\n",
    "        \"\"\"\n",
    "        Tokenizes a batch of examples.\n",
    "\n",
    "        Args:\n",
    "            examples (Dict[str, Any]): A dictionary of examples with keys 'text' and 'label'\n",
    "\n",
    "        Returns:\n",
    "            Tokenized examples ready for model training.\n",
    "        \"\"\"\n",
    "        tokens = self.tokenizer(\n",
    "            examples['text'], \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            max_length=self.max_span_length\n",
    "            )\n",
    "        tokens[\"labels\"] = examples[\"label\"]\n",
    "        return tokens\n",
    "\n",
    "    def compute_metrics(self, eval_pred):\n",
    "        \"\"\"\n",
    "        Compute metrics for the Hugging Face Trainer.\n",
    "\n",
    "        Args:\n",
    "            eval_pred: Named tuple containing predictions and labels.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Dictionary containing accuracy, precision, recall, F1 score, and class-specific metrics.\n",
    "        \"\"\"\n",
    "        labels = eval_pred.label_ids\n",
    "        preds = eval_pred.predictions.argmax(axis=-1)\n",
    "\n",
    "        # Mask ignored tokens (-100)\n",
    "        valid_indices = labels != -100\n",
    "        labels_flat = labels[valid_indices]\n",
    "        preds_flat = preds[valid_indices]\n",
    "\n",
    "        # Compute overall metrics\n",
    "        accuracy = accuracy_score(labels_flat, preds_flat)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            labels_flat, preds_flat, average=\"binary\", zero_division=1\n",
    "        )\n",
    "\n",
    "        # Per-Class Metrics\n",
    "        class_metrics = precision_recall_fscore_support(\n",
    "            labels_flat, preds_flat, average=None, labels=[0, 1]\n",
    "        )\n",
    "        non_propaganda_metrics = {\n",
    "            \"precision\": class_metrics[0][0], \n",
    "            \"recall\": class_metrics[1][0], \n",
    "            \"f1\": class_metrics[2][0]\n",
    "        }\n",
    "        propaganda_metrics = {\n",
    "            \"precision\": class_metrics[0][1], \n",
    "            \"recall\": class_metrics[1][1], \n",
    "            \"f1\": class_metrics[2][1]\n",
    "        }\n",
    "\n",
    "        # Optional Debug Logging for a Subset\n",
    "        if getattr(self, \"output_dir\", None):\n",
    "            debug_path = os.path.join(self.output_dir, \"debug_labels_preds.txt\")\n",
    "            sample_indices = np.random.choice(len(labels_flat), size=min(100, len(labels_flat)), replace=False)\n",
    "            with open(debug_path, \"w\") as f:\n",
    "                f.write(\"Subset of Predictions and True Labels:\\n\")\n",
    "                for i in sample_indices:\n",
    "                    f.write(f\"Index: {i}, True Label: {labels_flat[i]}, Predicted Label: {preds_flat[i]}\\n\")\n",
    "\n",
    "            # Classification report\n",
    "            report = classification_report(\n",
    "                labels_flat,\n",
    "                preds_flat,\n",
    "                target_names=[\"Non-Propaganda\", \"Propaganda\"]\n",
    "            )\n",
    "            report_path = os.path.join(self.output_dir, \"classification_report.txt\")\n",
    "            with open(report_path, \"w\") as f:\n",
    "                f.write(\"Detailed Classification Report:\\n\")\n",
    "                f.write(report)\n",
    "\n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"non_propaganda_metrics\": non_propaganda_metrics,\n",
    "            \"propaganda_metrics\": propaganda_metrics\n",
    "        }\n",
    "\n",
    "    def train(self, \n",
    "            train_articles_dir: str, \n",
    "            train_labels_dir: str, \n",
    "            test_size: float = 0.1,\n",
    "            learning_rate: float = 5e-5,\n",
    "            lr_decay_patience: int = 5):\n",
    "        \"\"\"\n",
    "        Train the propaganda detector using gradient accumulation.\n",
    "        \"\"\"\n",
    "        # Load tokenized dataset\n",
    "        tokenized_dataset = self.load_data(train_articles_dir, train_labels_dir)        \n",
    "        train_test_split = tokenized_dataset.train_test_split(test_size=test_size)\n",
    "\n",
    "        # Training arguments with gradient accumulation\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='models/output', \n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=1e-5, \n",
    "            per_device_train_batch_size=64 if torch.cuda.is_available() else 16,\n",
    "            per_device_eval_batch_size=64 if torch.cuda.is_available() else 16,\n",
    "            auto_find_batch_size=True,\n",
    "            num_train_epochs=200,\n",
    "            weight_decay=0.01,\n",
    "            gradient_accumulation_steps=2,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            logging_dir='models/output/logs', \n",
    "            logging_steps=5,\n",
    "            save_total_limit=15,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            fp16_opt_level=\"O1\",\n",
    "            gradient_checkpointing=True,  # reduce memory\n",
    "            max_grad_norm=1.0 # prevent gradient issues\n",
    "        )\n",
    "\n",
    "        # Data collator for efficient padding\n",
    "        data_collator = DataCollatorForTokenClassification(\n",
    "            tokenizer=self.tokenizer,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Optimizer and scheduler\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=lr_decay_patience, verbose=True)\n",
    "\n",
    "        # Function to update scheduler on evaluation\n",
    "        def update_scheduler(trainer, state, control, metrics=None, **kwargs):\n",
    "            if metrics and \"eval_loss\" in metrics:\n",
    "                scheduler.step(metrics[\"eval_loss\"])\n",
    "\n",
    "        # Trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_test_split[\"train\"],\n",
    "            eval_dataset=train_test_split[\"test\"],\n",
    "            compute_metrics=self.compute_metrics,  # No changes here\n",
    "            data_collator=data_collator,\n",
    "            optimizers=(optimizer, None),  # Scheduler handled separately\n",
    "            \n",
    "        )\n",
    "\n",
    "        # Train and evaluate\n",
    "        trainer.train()\n",
    "        trainer.evaluate()\n",
    "\n",
    "        # Save final model and tokenizer\n",
    "        trainer.save_model(os.path.join(self.output_dir, \"final_model\"))\n",
    "        self.tokenizer.save_pretrained(os.path.join(self.output_dir, \"final_model\"))\n",
    "\n",
    "\n",
    "    def predict(self, article_id: str, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Predict propaganda spans in the given text.\n",
    "\n",
    "        Args:\n",
    "            article_id (str): The ID of the article.\n",
    "            text (str): Input text to analyze.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: Predictions in the format \"article_id start end\".\n",
    "        \"\"\"\n",
    "\n",
    "        # Tokenize the input text\n",
    "        tokenized = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_span_length,\n",
    "            return_offsets_mapping=True,  # Get offsets for alignment\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Extract offset mapping and remove it from tokenized inputs\n",
    "        offset_mapping = tokenized.pop(\"offset_mapping\")\n",
    "\n",
    "        # Make predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**tokenized)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)  # Keep as a tensor for consistency\n",
    "\n",
    "        # Ensure predictions are a list (handles single vs batch cases)\n",
    "        predictions = predictions.squeeze().tolist()\n",
    "        if isinstance(predictions, int):  # Single prediction case\n",
    "            predictions = [predictions]\n",
    "\n",
    "        # Convert predictions back to text spans\n",
    "        propaganda_spans = []\n",
    "        offset_mapping = offset_mapping.squeeze().tolist()\n",
    "        if isinstance(offset_mapping[0], int):  # Handle single-token case\n",
    "            offset_mapping = [offset_mapping]\n",
    "\n",
    "        for idx, pred in enumerate(predictions):\n",
    "            if pred == 1:  # Predicted as propaganda\n",
    "                start, end = offset_mapping[idx]\n",
    "                propaganda_spans.append((start, end))\n",
    "        \n",
    "        # Merge consecutive spans\n",
    "        merged_spans = []\n",
    "        for start, end in propaganda_spans:\n",
    "            if not merged_spans or start > merged_spans[-1][1]:\n",
    "                merged_spans.append((start, end))\n",
    "            else:\n",
    "                # Extend the last span\n",
    "                merged_spans[-1] = (merged_spans[-1][0], max(merged_spans[-1][1], end))\n",
    "\n",
    "\n",
    "        # Format output as \"article_id start end\"\n",
    "        formatted_output = [f\"{article_id}\\t{start}\\t{end}\" for start, end in merged_spans]\n",
    "\n",
    "        return formatted_output\n",
    "\n",
    "    def predict_from_folder(self, folder_path: str, output_file: str) -> None:\n",
    "        \"\"\"\n",
    "        Predict propaganda spans for all articles in a folder and save the results.\n",
    "\n",
    "        Args:\n",
    "            folder_path (str): Path to the folder containing test article `.txt` files.\n",
    "            output_file (str): Path to the file where predictions will be saved.\n",
    "\n",
    "        Returns:\n",
    "            None: Writes predictions to the specified output file.\n",
    "        \"\"\"\n",
    "        all_predictions = []\n",
    "\n",
    "        # Process each article file in the folder\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.txt') and file_name.startswith('article'):\n",
    "                # Extract article ID from the file name\n",
    "                article_id = file_name[7:-4]  # Removes 'article' prefix and '.txt' suffix\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "                # Read the article text\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "\n",
    "                # Get predictions for this article\n",
    "                predictions = self.predict(article_id, text)\n",
    "                \n",
    "                # Concatenate continuous spans\n",
    "                merged_predictions = []\n",
    "                current_start, current_end = None, None\n",
    "                for prediction in predictions:\n",
    "                    _, start, end = prediction.split('\\t')\n",
    "                    start, end = int(start), int(end)\n",
    "\n",
    "                    if current_start is None:\n",
    "                        current_start, current_end = start, end\n",
    "                    elif start <= current_end + 1:  # Extend the current span\n",
    "                        current_end = max(current_end, end)\n",
    "                    else:  # Save the completed span\n",
    "                        merged_predictions.append(f\"{article_id}\\t{current_start}\\t{current_end}\")\n",
    "                        current_start, current_end = start, end\n",
    "\n",
    "                # Save the last span\n",
    "                if current_start is not None:\n",
    "                    merged_predictions.append(f\"{article_id}\\t{current_start}\\t{current_end}\")\n",
    "\n",
    "                all_predictions.extend(merged_predictions)\n",
    "\n",
    "        # Insert \"?\" as the second column for the propaganda type, necessary for pipeline into the second model\n",
    "        all_predictions = [line.split('\\t')[0] + '\\t?\\t' + '\\t'.join(line.split('\\t')[1:]) for line in all_predictions]\n",
    "\n",
    "        # Save all predictions to the output file\n",
    "        with open(output_file, \"w\") as f:\n",
    "            for line in all_predictions:\n",
    "                f.write(line + \"\\n\")\n",
    "\n",
    "        print(f\"Predictions saved to {output_file}\")\n",
    "\n",
    "    def save_predictions(self, output_file: str, predictions: List[str]):\n",
    "        \"\"\"\n",
    "        Save predictions to a file.\n",
    "\n",
    "        Args:\n",
    "            output_file (str): Path to the output file.\n",
    "            predictions (List[str]): List of predictions in the format \"article_id start end\".\n",
    "        \"\"\"\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(\"\\n\".join(predictions) + \"\\n\")\n",
    "\n",
    "\n",
    "# We used this function during development for debugging to fix misalignment of tokens and labels between prediction and training\n",
    "\n",
    "def test_tokenization_and_label_alignment_for_article(detector: PropagandaDetector, article_path: str, labels_path: str):\n",
    "    \"\"\"\n",
    "    Test if tokenization and label alignment during training match tokenization during inference\n",
    "    for a specific article.\n",
    "\n",
    "    Args:\n",
    "        detector (PropagandaDetector): The initialized detector instance.\n",
    "        article_path (str): Path to the article file.\n",
    "        labels_path (str): Path to the corresponding labels file.\n",
    "    \"\"\"\n",
    "    # Read the article content\n",
    "    with open(article_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Read the labels for the article\n",
    "    article_id = article_path.split('/')[-1].split('.')[0].replace('article', '')\n",
    "    spans = []\n",
    "    with open(labels_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if parts[0] == article_id:\n",
    "                spans.append((int(parts[1]), int(parts[2])))\n",
    "\n",
    "    print(f\"Article ID: {article_id}\")\n",
    "    print(f\"Text length: {len(text)}\")\n",
    "    print(f\"Propaganda spans: {spans}\")\n",
    "\n",
    "    # Tokenization and label alignment\n",
    "    training_tokenized = detector.extract_word_labels(text, spans)\n",
    "    print(\"\\n=== Training Tokenization & Labels ===\")\n",
    "    for token_id, label in zip(training_tokenized[\"input_ids\"], training_tokenized[\"labels\"]):\n",
    "        print(f\"Token ID: {token_id}, Token: {detector.tokenizer.decode([token_id])}, Label: {label}\")\n",
    "\n",
    "    # Inference tokenization\n",
    "    inference_tokenized = detector.tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=detector.max_span_length,\n",
    "        return_offsets_mapping=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    offsets = inference_tokenized.pop(\"offset_mapping\").squeeze().tolist()\n",
    "\n",
    "    print(\"\\n=== Inference Tokenization ===\")\n",
    "    for token_id, offset in zip(inference_tokenized[\"input_ids\"].squeeze().tolist(), offsets):\n",
    "        token = detector.tokenizer.decode([token_id])\n",
    "        print(f\"Token ID: {token_id}, Token: {token}, Offset: {offset}\")\n",
    "\n",
    "    print(\"\\n=== Comparison of Training and Inference ===\")\n",
    "    for idx, (train_id, train_label) in enumerate(zip(training_tokenized[\"input_ids\"], training_tokenized[\"labels\"])):\n",
    "        if idx < len(inference_tokenized[\"input_ids\"].squeeze()):\n",
    "            infer_id = inference_tokenized[\"input_ids\"].squeeze()[idx].item()\n",
    "            if train_id != infer_id:\n",
    "                print(f\"Mismatch at index {idx}: Training Token ID {train_id}, Inference Token ID {infer_id}\")\n",
    "            else:\n",
    "                print(f\"Match at index {idx}: Token ID {train_id}\")\n",
    "        else:\n",
    "            print(f\"Training token {train_id} exceeds inference tokenization length.\")\n",
    "\n",
    "    print(\"\\n=== Debugging Completed ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Analyses\n",
    "Model Performance\n",
    "\n",
    "The model was evaluated using a validation set, with the following metrics reported:\n",
    "\n",
    "    Precision: 37.95%\n",
    "    Recall: 42.20%\n",
    "    F1 Score: 39.96%\n",
    "\n",
    "These metrics demonstrate the model's capability to accurately identify propaganda spans in textual data, getting close to the competition winner's F1-score of around 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the PropagandaDetector class to load our pretrained DISTILBERT-BASE-UNCASED model from a checkpoint and create predictions of propaganda spans on the test articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "CUDA Available: False\n",
      "Predictions saved to predictions.txt\n"
     ]
    }
   ],
   "source": [
    "# Set up Model from checkpoint/new model \n",
    "detector = PropagandaDetector(\n",
    "    #model_name=\"distilbert-base-uncased\",                     # select new model to use\n",
    "    resume_from_checkpoint=\"models/output/final_model_monday\"  # or select from which checkpoint to resume training/predictions\n",
    ")\n",
    "\n",
    "# Paths to train and test articles and labels\n",
    "test_articles_dir = 'datasets/test-articles'\n",
    "train_articles_dir = 'datasets/train-articles'\n",
    "train_labels_dir = 'datasets/all_in_one_labels/all_labels.txt'\n",
    "test_labels_dir = 'datasets/test-task-tc-template.txt'\n",
    "\n",
    "\n",
    "# Paths for testing on only two articles\n",
    "subset_train_articles_dir = 'datasets/two-articles'\n",
    "subset_train_labels_dir = 'datasets/two-labels'\n",
    "\n",
    "output_predictions_file = \"predictions.txt\"\n",
    "\n",
    "# Create predictions on all articles in the test_articles_dir path folder, store predictions in output_predictions_file\n",
    "detector.predict_from_folder(test_articles_dir, output_predictions_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, a new model can be trained here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "print(\"Loading the model...\")\n",
    "detector = PropagandaDetector(\n",
    "    model_name=\"distilbert-base-uncased\"    # Customize to choose a different huggingface model\n",
    ")\n",
    "\n",
    "detector.train(train_articles_dir, train_labels_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions of the binary sequence tagging task performed above in combination with the original article texts are used as an input for the second task: Classifying the detected propaganda spans into their respective propaganda types.\n",
    "\n",
    "### Task 2: Propaganda Type Classification\n",
    "\n",
    "All relevant libraries have been imported above.\n",
    "\n",
    "First, we define the 14 classes in the `label_to_id` dict. Because some classes were combined they contain several (similar) types of propaganda: 2, 5, 12, 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the label-to-ID mapping\n",
    "label_to_id = {\n",
    "    \"O\": 0,  # Background or no-propaganda\n",
    "    \"Loaded_Language\": 1,\n",
    "    \"Name_Calling,Labeling\": 2,\n",
    "    \"Repetition\": 3,\n",
    "    \"Doubt\": 4,\n",
    "    \"Exaggeration,Minimisation\": 5,\n",
    "    \"Appeal_to_Authority\": 6,\n",
    "    \"Black-and-White_Fallacy\": 7,\n",
    "    \"Causal_Oversimplification\": 8,\n",
    "    \"Slogans\": 9,\n",
    "    \"Appeal_to_Fear-Prejudice\": 10,\n",
    "    \"Flag-Waving\": 11,\n",
    "    \"Whataboutism,Straw_Men,Red_Herring\": 12,\n",
    "    \"Thought-Terminating_Cliches\": 13,\n",
    "    \"Bandwagon,Reductio_ad_hitlerum\": 14\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``load_training_data`` loads and structures training data from text articles and their associated label files. The function consolidates article text and annotated propaganda spans into a dictionary format keyed by article IDs and containing start, end, and label (propaganda type) suitable for further processing by the extract_span_texts_and_labels function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(train_articles_dir, train_labels_dir):\n",
    "    \"\"\"\n",
    "    Loads the training articles and their associated propaganda spans.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_articles_dir : str\n",
    "        Path to the directory containing the training article text files.\n",
    "    train_labels_dir : str\n",
    "        Path to the directory containing the label files.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary keyed by article ID, each value is a dictionary containing:\n",
    "        {\n",
    "            \"text\": str,  # full article text\n",
    "            \"spans\": [\n",
    "                {\n",
    "                    \"label\": str,        # propaganda type\n",
    "                    \"start\": int,        # start index of span\n",
    "                    \"end\": int           # end index of span\n",
    "                },\n",
    "                ...\n",
    "            ]\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    # Dictionary to hold loaded data\n",
    "    data = {}\n",
    "\n",
    "    # The articles follow a pattern like \"article<id>.txt\"\n",
    "    article_files = glob.glob(os.path.join(train_articles_dir, \"article*.txt\"))\n",
    "\n",
    "    for article_path in article_files:\n",
    "        # Extract the article ID from the filename\n",
    "        basename = os.path.basename(article_path)\n",
    "        # Example filename: article111111111.txt -> article ID: 111111111\n",
    "        article_id = basename.replace(\"article\", \"\").replace(\".txt\", \"\")\n",
    "\n",
    "        # Load the article text\n",
    "        with open(article_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            article_text = f.read()\n",
    "\n",
    "        # Initialize the structure in the dictionary\n",
    "        data[article_id] = {\n",
    "            \"text\": article_text,\n",
    "            \"spans\": []\n",
    "        }\n",
    "\n",
    "        # Corresponding label file should be: article<articleid>.task2-TC.labels\n",
    "        label_file = os.path.join(train_labels_dir, f\"article{article_id}.task2-TC.labels\")\n",
    "\n",
    "        if os.path.exists(label_file):\n",
    "            with open(label_file, \"r\", encoding=\"utf-8\") as lf:\n",
    "                for line in lf:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    # Each line format: articleid  label  start  end\n",
    "                    parts = line.split('\\t')\n",
    "                    if len(parts) != 4:\n",
    "                        continue\n",
    "                    _, label, start_str, end_str = parts\n",
    "                    start = int(start_str)\n",
    "                    end = int(end_str)\n",
    "                    \n",
    "                    data[article_id][\"spans\"].append({\n",
    "                        \"label\": label,\n",
    "                        \"start\": start,\n",
    "                        \"end\": end\n",
    "                    })\n",
    "        else:\n",
    "            # If there's no label file, continue (some articles may not have propaganda spans)\n",
    "            pass\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``extract_span_texts_and_labels`` processes the dictionary of training data returned by load_training_data. The function extracts propaganda spans and their corresponding labels, preparing the data for training by returning separate lists for span texts and associated labels. Unlike in task 1, only those sections labelled as containing a specific type of propaganda are retained. The function skips articles without spans and handles scenarios with overlapping spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_span_texts_and_labels(training_data):\n",
    "    \"\"\"\n",
    "    Given a dictionary of training data as returned by load_training_data,\n",
    "    extract the propaganda span texts and their corresponding labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    training_data : dict\n",
    "        Dictionary keyed by article ID, where each value is a dictionary:\n",
    "        {\n",
    "            \"text\": str (full article text),\n",
    "            \"spans\": [\n",
    "                {\n",
    "                    \"label\": str,   # propaganda type\n",
    "                    \"start\": int,   # start index of span in chars\n",
    "                    \"end\": int      # end index of span in chars\n",
    "                },\n",
    "                ...\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    span_texts : list of str\n",
    "        A list of extracted span texts from the articles.\n",
    "    span_labels : list of str\n",
    "        A list of corresponding labels for each extracted span text.\n",
    "    \"\"\"\n",
    "    span_texts = []\n",
    "    span_labels = []\n",
    "    \n",
    "    for article_id, content in training_data.items():\n",
    "        article_text = content[\"text\"]\n",
    "        spans = content[\"spans\"]\n",
    "        print(f\"\\nArticle ID: {article_id}\")\n",
    "        print(f\"Article text length: {len(article_text)} chars\")\n",
    "        print(f\"Number of spans found: {len(spans)}\")\n",
    "        \n",
    "        for i, span in enumerate(spans):\n",
    "            start = span[\"start\"]\n",
    "            end = span[\"end\"]\n",
    "            label = span[\"label\"]\n",
    "            span_text = article_text[start:end]\n",
    "            span_texts.append(span_text)\n",
    "            span_labels.append(label)\n",
    "\n",
    "            # Print a few example spans\n",
    "            if i < 3:\n",
    "                print(f\"  Example span {i+1}: '{span_text[:50]}...', Label: {label}, Start: {start}, End: {end}\")\n",
    "\n",
    "    print(\"\\nTotal extracted spans:\", len(span_texts))\n",
    "    if len(span_texts) > 0:\n",
    "        print(\"First 3 extracted spans:\", span_texts[:3])\n",
    "        print(\"First 3 extracted labels:\", span_labels[:3])\n",
    "\n",
    "    return span_texts, span_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``predict`` applies a trained model to classify propaganda types for text spans within articles. It processes inputs from an article folder and a span file, loading and associating article text with specified spans (such as those found by the first model). The function tokenizes each span, performs inference using the fine-tuned DistilBERT model, and outputs predictions in a standardized format. It manages missing articles, filters invalid span data, and utilizes GPU acceleration for efficiency. Predictions are saved to an output file for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(article_folder: str, span_file: str, model_path: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Predicts propaganda types for spans in articles using a trained model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    article_folder : str\n",
    "        Path to the folder containing article text files.\n",
    "    span_file : str\n",
    "        Path to the file containing spans to predict in the format:\n",
    "        <article_id>    <irrelevant_column>    <start>    <end>\n",
    "    model_path : str\n",
    "        Path to the trained model for prediction.\n",
    "    output_file : str\n",
    "        Path to save the predictions in the format:\n",
    "        <article_id>    <start>    <end>    <predicted_label>\n",
    "    \"\"\"\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load articles\n",
    "    print(\"Loading articles...\")\n",
    "    articles = {}\n",
    "    for file in os.listdir(article_folder):\n",
    "        if file.startswith(\"article\") and file.endswith(\".txt\"):\n",
    "            article_id = file.replace(\"article\", \"\").replace(\".txt\", \"\")\n",
    "            with open(os.path.join(article_folder, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                articles[article_id] = f.read()\n",
    "\n",
    "    print(f\"Loaded {len(articles)} articles.\")\n",
    "\n",
    "    # Load spans\n",
    "    print(\"Loading spans...\")\n",
    "    spans = []\n",
    "    with open(span_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) >= 4:  # Ensure there are at least 4 columns\n",
    "                article_id, _, start, end = parts\n",
    "                spans.append((article_id, int(start), int(end)))\n",
    "            else:\n",
    "                print(f\"Skipping invalid line: {line.strip()}\")\n",
    "\n",
    "    print(f\"Loaded {len(spans)} spans.\")\n",
    "\n",
    "    # Predict propaganda type for each span\n",
    "    print(\"Predicting propaganda types...\")\n",
    "    predictions = []\n",
    "    for article_id, start, end in spans:\n",
    "        if article_id in articles:\n",
    "            article_text = articles[article_id]\n",
    "            span_text = article_text[start:end]\n",
    "\n",
    "            # Tokenize the span\n",
    "            inputs = tokenizer(span_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "            inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "            # Get model predictions\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                predicted_label_id = torch.argmax(logits, dim=-1).item()\n",
    "                predicted_label = model.config.id2label[predicted_label_id]\n",
    "\n",
    "            # Save the prediction\n",
    "            predictions.append((article_id, start, end, predicted_label))\n",
    "        else:\n",
    "            print(f\"Warning: Article ID {article_id} not found.\")\n",
    "\n",
    "    # Save predictions to output file\n",
    "    print(f\"Saving predictions to {output_file}...\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for article_id, start, end, predicted_label in predictions:\n",
    "            f.write(f\"{article_id}\\t{start}\\t{end}\\t{predicted_label}\\n\")\n",
    "\n",
    "    print(\"Predictions saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``compute_metrics`` evaluates model performance by calculating accuracy, precision, recall, and F1-score. It processes predictions and input labels from *EvalPrediction*, extracting predicted classes by selecting the highest-scoring **logits**. Metrics are computed using macro averaging to ensure equal weighting of all classes, regardless of class imbalance. The function returns a dictionary mapping metric names to their respective values, for evaluation during training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    \"\"\"\n",
    "    Computes metrics for evaluation during training and testing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : EvalPrediction\n",
    "        Contains `predictions` and `label_ids`:\n",
    "        - `predictions` is a 2D array of shape (n_samples, n_classes) with the logits or probabilities.\n",
    "        - `label_ids` is a 1D array of shape (n_samples,) with the ground-truth labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary mapping metric names to their values.\n",
    "    \"\"\"\n",
    "    preds = p.predictions\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    # Get the predicted class by choosing the class with the highest logit score\n",
    "    preds = preds.argmax(axis=1)\n",
    "    labels = p.label_ids\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "\n",
    "    # Compute precision, recall, f1 for each class\n",
    "    # average='macro' calculates metrics independently for each class \n",
    "    # and then takes the average of them\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code establishes a pipeline for the labelled labeled data for training. It verifies GPU availability, the training and testing data paths are redefined (in case different data is used than for task 1), and a subset option is provided for debugging or testing on a smaller dataset. \n",
    "\n",
    "Articles and labels are loaded using ``load_training_data``. Propaganda spans and their labels are extracted using ``extract_span_texts_and_labels``, forming the basis for model inputs. The class labels are mapped to their corresponding integers for compatibility with the transformers library. Spans are tokenized with the DistilBERT tokenizer, converting text into input features suitable for the model, including truncation and padding. The ``PropagandaTypeDataset`` class organizes tokenized data and labels into a PyTorch-compatible format, to be used for batching during training. Finally, the training dataset is divided into training (90%) and validation (10%) subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify CUDA availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths to train and test articles and labels folders\n",
    "test_articles_dir = 'datasets/test-articles'\n",
    "train_articles_dir = 'datasets/train-articles'\n",
    "test_labels_dir = 'datasets/test-task-tc-template.txt'\n",
    "train_labels_dir = 'datasets/train-labels-task2-technique-classification'\n",
    "\n",
    "# Paths for testing on only two articles\n",
    "subset_train_articles_dir = 'datasets/two-articles'\n",
    "subset_train_labels_dir = 'datasets/two-labels-task2'\n",
    "output_predictions_file = \"predictions_task2.txt\"\n",
    "\n",
    "testrundata = load_training_data(train_articles_dir, train_labels_dir)\n",
    "span_texts, span_labels = extract_span_texts_and_labels(testrundata)\n",
    "\n",
    "# Create a label map\n",
    "unique_labels = sorted(set(span_labels))\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# Convert labels to integers\n",
    "numeric_labels = [label2id[l] for l in span_labels]\n",
    "\n",
    "# Tokenize the data\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "encodings = tokenizer(span_texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Create a Torch Dataset\n",
    "class PropagandaTypeDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "dataset = PropagandaTypeDataset(encodings, numeric_labels)\n",
    "\n",
    "# Split into train/validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code fine-tunes the chosen DistilBERT model using efficient training practices . It begins by loading the model to the device (CPU or GPU), configured with task-specific parameters like the number of labels and mappings between label IDs and names as previously defined. The DistilBERT model is prepared with ignore_mismatched_sizes to accommodate potential differences in pretrained and task-specific configurations. The model is trained and evaluated per epoch, with metrics printed to assess performance. The best model is loaded automatically at the end of training and saved with the tokenizer for future use.\n",
    "\n",
    "An ``early_stopping_callback`` halts training if performance does not improve after 10 evaluations, preventing overfitting.\n",
    "\n",
    "Hyperparameters and logging settings are defined in ``TrainingArguments``. These include batch sizes, learning rate, gradient accumulation for memory optimization, and cosine decay for the learning rate. **F1 score** is selected as the metric to identify the best model as it balances precision and recall, making it more useful for our task where classes are not perfectly evenly represented.\n",
    "\n",
    "The ``Trainer`` class is used to streamline training, integrating the model, datasets, training arguments, and the compute_metrics function for evaluating performance.\n",
    "\n",
    "The ``predict`` function as defined above is applied using the trained model to classify spans, generating predictions in a .txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=len(unique_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ").to(device)\n",
    "\n",
    "# Set up early stopping to halt training if no improvement after 10 evaluations\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=10)\n",
    "\n",
    "# Define training arguments, including hyperparameters and logging settings\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results-task2',         \n",
    "    num_train_epochs=100,                 # Number of training epochs\n",
    "    per_device_train_batch_size=64,       # Batch size for training\n",
    "    per_device_eval_batch_size=64,        # Batch size for evaluation\n",
    "    warmup_steps=100,                     # Warmup steps for learning rate scheduler\n",
    "    weight_decay=0.02,                    # Double the weight decay as in the previous task because of the fewer epochs\n",
    "    eval_strategy=\"epoch\",                \n",
    "    save_strategy=\"epoch\",                \n",
    "    logging_dir='./logs-task2',           \n",
    "    logging_steps=50,                     # Log every 50 steps\n",
    "    save_total_limit=50,                  # Keep the last 50 models\n",
    "    load_best_model_at_end=True,          # Load the best model after training\n",
    "    report_to=\"none\",                     # Disable reporting to external tools\n",
    "    gradient_accumulation_steps=3,        # Accumulate gradients over 3 steps to reduce memory usage with \"larger\" batch sizes\n",
    "    metric_for_best_model=\"f1\",           # Use F1 score to determine best model\n",
    "    learning_rate=5e-5,                   # Initial learning rate\n",
    "    lr_scheduler_type=\"cosine\",           # Use cosine decay for the learning rate\n",
    ")\n",
    "\n",
    "# Initialize the Trainer with model, arguments, datasets, and early stopping\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model and print evaluation metrics\n",
    "eval_metrics = trainer.evaluate()\n",
    "print(\"Evaluation metrics:\", eval_metrics)\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "trainer.save_model(\"./trained_model_task2\")\n",
    "tokenizer.save_pretrained(\"./trained_model_task2\")\n",
    "\n",
    "# Run predictions using the trained model\n",
    "task2labelspath = 'datasets/train-task2-TC.labels'\n",
    "predict(train_articles_dir, task2labelspath, model_name, output_predictions_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for task 2: Span Technique Classification\n",
    "\n",
    "The model was evaluated using a validation set, with the following metrics reported:\n",
    "\n",
    "    Precision: 59.11%\n",
    "    Recall: 53.15%\n",
    "    F1 Score: 52.49%\n",
    "\n",
    "### Conclusion / Discussion\n",
    "\n",
    "Our project demonstrates that transformer-based models can effectively detect and classify propaganda in news articles, though significant challenges remain. We experimented with different models from the RoBERTa, LLaMA, and BERT families, and ended up using DistilBERT because of the limited available computational resources. We achieved an F1 score of 0.40 for task 1 span identification and 0.53 for task 2 propaganda technique classification, aligning with average scores of previously reported ranges in the literature of 0.18-0.50 and 0.20-0.60, respectively (Da San Martino et al., 2020).\n",
    "\n",
    "The results are particularly noteworthy given our use of a smaller model (DistilBERT, 66M parameters) compared to previous work that employed larger models like the BERT base model (110M), XLNet (340M), or XLM RoBERTa (355M), usually used in an ensemble method. This suggests that even resource-efficient approaches can achieve competitive performance in propaganda detection tasks and classification tasks.\n",
    "The complexity of propaganda detection stems from several factors. Unlike simpler text classification tasks, identifying propagandistic content requires understanding context and recognizing author intent. Additionally, propaganda techniques vary widely, from simple loaded language to complex logical fallacies, making their automated detection particularly challenging.\n",
    "\n",
    "While our model performs adequately on English-language news articles from 2017-2019, its generalizability to more recent content remains untested. Several opportunities exist for improving performance:\n",
    "\n",
    "First, implementing more modern models like ModernBERT (released on 19/12/2024) or larger models like llama 3.1 could enhance detection accuracy. Second, expanding the training dataset with more recent news articles would help the model stay current with evolving propaganda techniques and language patterns. Finally, following the example of projects like Tanbih, incorporating zero-shot classification capabilities and multi-language support would increase the model's utility.\n",
    "\n",
    "### Division of Labour:\n",
    "\n",
    "Conrad: Data loading (minority), metrics, training setup, CUDA, hyperparameter tuning, poster making, elevator pitch, Introduction, Conclusion/Discussion, function descriptions task 2\n",
    "\n",
    "Robin: Project architecture, data pipeline, PropagandaDetector conceptualization + coding except metrics, task 2 conceptualization + coding, debugging, poster making, elevator pitch, notebook code part\n",
    "\n",
    "### Use of AI\n",
    "\n",
    "The code was written by us. ChatGPT and Claude were used as debuggers, code optimizers and for function descriptions. \n",
    "\n",
    "### Note on notebook\n",
    "\n",
    "We used scripts in a GitHub repository during the duration the development of this project and consolidated the code into this notebook for delivery. This is the reason why there is a large class for task 1 which cannot be broken down well into smaller code cells without rewriting the code.\n",
    "\n",
    "### Sources\n",
    "\n",
    "Jurkiewicz, D., Borchmann, Ł., Kosmala, I., & Graliński, F. (2020). ApplicaAI at SemEval-2020 task 11: On RoBERTa-CRF, span CLS and whether self-training helps them. arXiv preprint arXiv:2005.07934.\n",
    "\n",
    "Martino, G., Barrón-Cedeno, A., Wachsmuth, H., Petrov, R., & Nakov, P. (2020). SemEval-2020 task 11: Detection of propaganda techniques in news articles. arXiv preprint arXiv:2009.02696.\n",
    "\n",
    "https://huggingface.co/distilbert/distilbert-base-uncased\n",
    "\n",
    "https://ground.news/\n",
    "\n",
    "https://tanbih.qcri.org/publications/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
